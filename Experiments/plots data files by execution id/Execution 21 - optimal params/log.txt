Logging to /tmp/openaiGA
T: 10
_Q_lr: 0.001
_action_l2: 1.0
_batch_size: 256
_buffer_size: 1000000
_clip_obs: 200.0
_hidden: 256
_layers: 3
_max_u: 1.0
_network_class: baselines.her.actor_critic:ActorCritic
_norm_clip: 5
_norm_eps: 0.01
_pi_lr: 0.001
_polyak: 0.184
_relative_goals: False
_scope: ddpg
ddpg_params: {'buffer_size': 1000000, 'hidden': 256, 'layers': 3, 'network_class': 'baselines.her.actor_critic:ActorCritic', 'polyak': 0.184, 'batch_size': 256, 'Q_lr': 0.001, 'pi_lr': 0.001, 'norm_eps': 0.01, 'norm_clip': 5, 'max_u': 1.0, 'action_l2': 1.0, 'clip_obs': 200.0, 'scope': 'ddpg', 'relative_goals': False}
env_name: AuboReach-v1
gamma: 0.88
make_env: <function prepare_params.<locals>.make_env at 0x7f1157fccf80>
n_batches: 20
n_cycles: 25
n_test_rollouts: 20
noise_eps: 0.774
random_eps: 0.055
replay_k: 4
replay_strategy: future
rollout_batch_size: 2
test_with_polyak: False
Creating a DDPG agent with action space 6 x 1.0...
Training...
--------------------------------------------
| epoch              | 0                   |
| stats_g/mean       | 0.0058762482        |
| stats_g/std        | 0.6180453           |
| stats_o/mean       | 0.39371264          |
| stats_o/std        | 0.027805978         |
| test/episode       | 40.0                |
| test/mean_Q        | -8.272127           |
| test/success_rate  | 0.325               |
| train/episode      | 50.0                |
| train/success_rate | 0.20000000000000004 |
--------------------------------------------
New best success rate: 0.325. Saving policy to /tmp/openaiGA/policy_best.pkl ...
Saving periodic policy to /tmp/openaiGA/policy_0.pkl ...
--------------------------------------------
| epoch              | 1                   |
| stats_g/mean       | 0.05035299          |
| stats_g/std        | 0.64505416          |
| stats_o/mean       | 0.3941059           |
| stats_o/std        | 0.021385612         |
| test/episode       | 80.0                |
| test/mean_Q        | -8.299514           |
| test/success_rate  | 0.25000000000000006 |
| train/episode      | 100.0               |
| train/success_rate | 0.25833333333333336 |
--------------------------------------------
--------------------------------------------
| epoch              | 2                   |
| stats_g/mean       | 0.061510667         |
| stats_g/std        | 0.6505285           |
| stats_o/mean       | 0.3942372           |
| stats_o/std        | 0.018534284         |
| test/episode       | 120.0               |
| test/mean_Q        | -8.311599           |
| test/success_rate  | 0.35833333333333334 |
| train/episode      | 150.0               |
| train/success_rate | 0.2644444444444444  |
--------------------------------------------
New best success rate: 0.35833333333333334. Saving policy to /tmp/openaiGA/policy_best.pkl ...
--------------------------------------------
| epoch              | 3                   |
| stats_g/mean       | 0.07734559          |
| stats_g/std        | 0.65873396          |
| stats_o/mean       | 0.39430293          |
| stats_o/std        | 0.016839856         |
| test/episode       | 160.0               |
| test/mean_Q        | -8.324657           |
| test/success_rate  | 0.30416666666666664 |
| train/episode      | 200.0               |
| train/success_rate | 0.2866666666666667  |
--------------------------------------------
------------------------------------
| epoch              | 4           |
| stats_g/mean       | 0.08007439  |
| stats_g/std        | 0.6632169   |
| stats_o/mean       | 0.39434233  |
| stats_o/std        | 0.015684022 |
| test/episode       | 200.0       |
| test/mean_Q        | -8.286593   |
| test/success_rate  | 0.3625      |
| train/episode      | 250.0       |
| train/success_rate | 0.32        |
------------------------------------
New best success rate: 0.3625. Saving policy to /tmp/openaiGA/policy_best.pkl ...
